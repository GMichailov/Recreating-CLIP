{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17210a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parent.parent\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d71532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6301ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.text_encoders import ClipDistilBert, ClipMiniLM\n",
    "from models.vision_encoders import ClipVitTiny, ClipVitSmall\n",
    "from models.ResNet50 import ClipResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4138a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_text_model_information(model_class, output_dim=256, device=\"cuda\"):\n",
    "    print(f\"MODEL: {model_class.__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "    model = model_class(output_dim).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU Memory Before Loading: {mem_before / 1e6:.2f} MB\")\n",
    "    print(f\"GPU Memory After Loading: {mem_after / 1e6:.2f} MB\")\n",
    "    print(f\"Model Memory Footprint: {(mem_after - mem_before) / 1e6:.2f} MB\\n\")\n",
    "    caption = \"This is a fake image.\"\n",
    "    t0 = time.perf_counter()\n",
    "    fake_caption = model.tokenizer(\n",
    "        caption,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        _ = model(**fake_caption)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Tokenization Time: {(t1 - t0)*1000:.2f} ms\")\n",
    "    print(f\"Forward Pass Time (BF16): {(t2 - t1)*1000:.2f} ms\")\n",
    "    print(f\"Total Pipeline Time: {(t2 - t0)*1000:.2f} ms\")\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Peak GPU Memory Used During Run: {peak_mem / 1e6:.2f} MB\\n\")\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"mem_before\": mem_before,\n",
    "        \"mem_after\": mem_after,\n",
    "        \"peak_mem\": peak_mem,\n",
    "        \"tokenize_time_ms\": (t1 - t0)*1000,\n",
    "        \"forward_time_ms\": (t2 - t1)*1000,\n",
    "        \"total_time_ms\": (t2 - t0)*1000,\n",
    "    }\n",
    "\n",
    "def log_vision_encoder_info(encoder_class, output_dim=256, device=\"cuda\"):\n",
    "    print(f\"VISION MODEL: {encoder_class.__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "    model = encoder_class(output_dim).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU Memory Before Loading: {mem_before / 1e6:.2f} MB\")\n",
    "    print(f\"GPU Memory After Loading: {mem_after / 1e6:.2f} MB\")\n",
    "    print(f\"Model Memory Footprint: {(mem_after - mem_before) / 1e6:.2f} MB\\n\")\n",
    "    raw_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "    t0 = time.perf_counter()\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        _ = model(raw_image)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Preprocessing Time: {(t1 - t0)*1000:.2f} ms\")\n",
    "    print(f\"Forward Pass Time (BF16): {(t2 - t1)*1000:.2f} ms\")\n",
    "    print(f\"Total Pipeline Time: {(t2 - t0)*1000:.2f} ms\")\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Peak GPU Memory Used During Run: {peak_mem / 1e6:.2f} MB\\n\")\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"mem_before\": mem_before,\n",
    "        \"mem_after\": mem_after,\n",
    "        \"peak_mem\": peak_mem,\n",
    "        \"preprocess_ms\": (t1 - t0)*1000,\n",
    "        \"forward_ms\": (t2 - t1)*1000,\n",
    "        \"total_ms\": (t2 - t0)*1000,\n",
    "    }\n",
    "\n",
    "def log_resnet_encoder_info(resnet_class, output_dim=256, device=\"cuda\"):\n",
    "    print(f\"VISION MODEL: {resnet_class.__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "    model = resnet_class(output_dim).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU Memory Before Loading: {mem_before / 1e6:.2f} MB\")\n",
    "    print(f\"GPU Memory After Loading: {mem_after / 1e6:.2f} MB\")\n",
    "    print(f\"Model Memory Footprint: {(mem_after - mem_before) / 1e6:.2f} MB\\n\")\n",
    "    raw_image = (torch.rand(224, 224, 3) * 255).to(torch.uint8)\n",
    "    t0 = time.perf_counter()\n",
    "    processed = raw_image.permute(2, 0, 1).float() / 255.0\n",
    "    processed = processed.unsqueeze(0).to(device)\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        _ = model(processed)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Preprocessing Time: {(t1 - t0)*1000:.2f} ms\")\n",
    "    print(f\"Forward Pass Time (BF16): {(t2 - t1)*1000:.2f} ms\")\n",
    "    print(f\"Total Pipeline Time: {(t2 - t0)*1000:.2f} ms\")\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Peak GPU Memory Used During Run: {peak_mem / 1e6:.2f} MB\\n\")\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"mem_before\": mem_before,\n",
    "        \"mem_after\": mem_after,\n",
    "        \"peak_mem\": peak_mem,\n",
    "        \"preprocess_ms\": (t1 - t0)*1000,\n",
    "        \"forward_ms\": (t2 - t1)*1000,\n",
    "        \"total_ms\": (t2 - t0)*1000,\n",
    "    }\n",
    "\n",
    "def log_information():\n",
    "    for text_encoder in [ClipDistilBert, ClipMiniLM]:\n",
    "        log_text_model_information(text_encoder)\n",
    "    for vision_encoder in [ClipVitTiny, ClipVitSmall]:\n",
    "        log_vision_encoder_info(vision_encoder)\n",
    "    #log_resnet_encoder_info(ClipResNet50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb07a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: ClipDistilBert\n",
      "Total Parameters: 66,559,744\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 344.58 MB\n",
      "Model Memory Footprint: 266.85 MB\n",
      "\n",
      "Tokenization Time: 1.74 ms\n",
      "Forward Pass Time (BF16): 29.15 ms\n",
      "Total Pipeline Time: 30.90 ms\n",
      "Peak GPU Memory Used During Run: 446.48 MB\n",
      "\n",
      "MODEL: ClipMiniLM\n",
      "Total Parameters: 22,811,776\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 169.64 MB\n",
      "Model Memory Footprint: 91.91 MB\n",
      "\n",
      "Tokenization Time: 0.90 ms\n",
      "Forward Pass Time (BF16): 11.93 ms\n",
      "Total Pipeline Time: 12.84 ms\n",
      "Peak GPU Memory Used During Run: 195.59 MB\n",
      "\n",
      "VISION MODEL: ClipVitTiny\n",
      "Total Parameters: 5,573,824\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 100.04 MB\n",
      "Model Memory Footprint: 22.32 MB\n",
      "\n",
      "Preprocessing Time: 0.00 ms\n",
      "Forward Pass Time (BF16): 15.06 ms\n",
      "Total Pipeline Time: 15.07 ms\n",
      "Peak GPU Memory Used During Run: 128.76 MB\n",
      "\n",
      "VISION MODEL: ClipVitSmall\n",
      "Total Parameters: 21,764,224\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 165.44 MB\n",
      "Model Memory Footprint: 87.71 MB\n",
      "\n",
      "Preprocessing Time: 0.00 ms\n",
      "Forward Pass Time (BF16): 36.51 ms\n",
      "Total Pipeline Time: 36.51 ms\n",
      "Peak GPU Memory Used During Run: 244.42 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_information()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd5fc8",
   "metadata": {},
   "source": [
    "### Text Encoder Comparison\n",
    "\n",
    "DistilBert vs MiniLM\n",
    "Parameters: 66M vs 22M\n",
    "Forward pass time: 29.15 vs 11.93 ms\n",
    "Memory Footprint: 266.85 vs 91.91 MB\n",
    "\n",
    "Due to the size of MiniLM, it may struggle to have a similar expressive capability as DistilBert.\n",
    "\n",
    "MiniLM is 2.4x faster and 2.9x lighter in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dacb5",
   "metadata": {},
   "source": [
    "### Vision Encoder Comparison\n",
    "\n",
    "VitTiny vs VitSmall\n",
    "Parameters: 5.5M vs 21.8M\n",
    "Forward Pass: 15.06 ms vs 36.51ms\n",
    "Memory Footprint: 22.32 MB vs 87.71 MB\n",
    "\n",
    "VitTiny is extremely small which is potentially concerning. Most likely, ViTSmall will end up being used or even potentially upscaling to the next size.\n",
    "\n",
    "ViTTiny is 2.4x faster and 3.9x lighter in memory. If ViTTiny works, it will be very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b04587",
   "metadata": {},
   "source": [
    "### Determining if a model works\n",
    "\n",
    "1. Compare the gradient norms. If one encoder receives much smaller gradients than the other, its capacity is too small and the other encoder is overcompensating.\n",
    "2. Can Freeze one encoder and see how the loss changes. If the loss barely decreases, the non-frozen model is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b75b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train_cycle(model):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
