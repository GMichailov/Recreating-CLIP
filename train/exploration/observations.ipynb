{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17210a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parent.parent\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6301ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.text_encoders import ClipDistilBert, ClipMiniLM\n",
    "from models.vision_encoders import ClipVitTiny, ClipVitSmall\n",
    "from models.ResNet50 import ClipResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4138a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_text_model_information(model_class, output_dim=256, device=\"cuda\"):\n",
    "    print(f\"MODEL: {model_class.__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "    model = model_class(output_dim).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU Memory Before Loading: {mem_before / 1e6:.2f} MB\")\n",
    "    print(f\"GPU Memory After Loading: {mem_after / 1e6:.2f} MB\")\n",
    "    print(f\"Model Memory Footprint: {(mem_after - mem_before) / 1e6:.2f} MB\\n\")\n",
    "    caption = \"This is a fake image.\"\n",
    "    t0 = time.perf_counter()\n",
    "    fake_caption = model.tokenizer(\n",
    "        caption,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        _ = model(**fake_caption)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Tokenization Time: {(t1 - t0)*1000:.2f} ms\")\n",
    "    print(f\"Forward Pass Time (BF16): {(t2 - t1)*1000:.2f} ms\")\n",
    "    print(f\"Total Pipeline Time: {(t2 - t0)*1000:.2f} ms\")\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Peak GPU Memory Used During Run: {peak_mem / 1e6:.2f} MB\\n\")\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"mem_before\": mem_before,\n",
    "        \"mem_after\": mem_after,\n",
    "        \"peak_mem\": peak_mem,\n",
    "        \"tokenize_time_ms\": (t1 - t0)*1000,\n",
    "        \"forward_time_ms\": (t2 - t1)*1000,\n",
    "        \"total_time_ms\": (t2 - t0)*1000,\n",
    "    }\n",
    "\n",
    "def log_vision_encoder_info(encoder_class, output_dim=256, device=\"cuda\"):\n",
    "    print(f\"VISION MODEL: {encoder_class.__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "    model = encoder_class(output_dim).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU Memory Before Loading: {mem_before / 1e6:.2f} MB\")\n",
    "    print(f\"GPU Memory After Loading: {mem_after / 1e6:.2f} MB\")\n",
    "    print(f\"Model Memory Footprint: {(mem_after - mem_before) / 1e6:.2f} MB\\n\")\n",
    "    raw_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "    t0 = time.perf_counter()\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        _ = model(raw_image)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Preprocessing Time: {(t1 - t0)*1000:.2f} ms\")\n",
    "    print(f\"Forward Pass Time (BF16): {(t2 - t1)*1000:.2f} ms\")\n",
    "    print(f\"Total Pipeline Time: {(t2 - t0)*1000:.2f} ms\")\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Peak GPU Memory Used During Run: {peak_mem / 1e6:.2f} MB\\n\")\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"mem_before\": mem_before,\n",
    "        \"mem_after\": mem_after,\n",
    "        \"peak_mem\": peak_mem,\n",
    "        \"preprocess_ms\": (t1 - t0)*1000,\n",
    "        \"forward_ms\": (t2 - t1)*1000,\n",
    "        \"total_ms\": (t2 - t0)*1000,\n",
    "    }\n",
    "\n",
    "def log_resnet_encoder_info(resnet_class, output_dim=256, device=\"cuda\"):\n",
    "    print(f\"VISION MODEL: {resnet_class.__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "    model = resnet_class(output_dim).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    print(f\"GPU Memory Before Loading: {mem_before / 1e6:.2f} MB\")\n",
    "    print(f\"GPU Memory After Loading: {mem_after / 1e6:.2f} MB\")\n",
    "    print(f\"Model Memory Footprint: {(mem_after - mem_before) / 1e6:.2f} MB\\n\")\n",
    "    raw_image = (torch.rand(224, 224, 3) * 255).to(torch.uint8)\n",
    "    t0 = time.perf_counter()\n",
    "    processed = raw_image.permute(2, 0, 1).float() / 255.0\n",
    "    processed = processed.unsqueeze(0).to(device)\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        _ = model(processed)\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Preprocessing Time: {(t1 - t0)*1000:.2f} ms\")\n",
    "    print(f\"Forward Pass Time (BF16): {(t2 - t1)*1000:.2f} ms\")\n",
    "    print(f\"Total Pipeline Time: {(t2 - t0)*1000:.2f} ms\")\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    print(f\"Peak GPU Memory Used During Run: {peak_mem / 1e6:.2f} MB\\n\")\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"mem_before\": mem_before,\n",
    "        \"mem_after\": mem_after,\n",
    "        \"peak_mem\": peak_mem,\n",
    "        \"preprocess_ms\": (t1 - t0)*1000,\n",
    "        \"forward_ms\": (t2 - t1)*1000,\n",
    "        \"total_ms\": (t2 - t0)*1000,\n",
    "    }\n",
    "\n",
    "def log_information():\n",
    "    for text_encoder in [ClipDistilBert, ClipMiniLM]:\n",
    "        log_text_model_information(text_encoder)\n",
    "    for vision_encoder in [ClipVitTiny, ClipVitSmall]:\n",
    "        log_vision_encoder_info(vision_encoder)\n",
    "    #log_resnet_encoder_info(ClipResNet50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb07a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: ClipDistilBert\n",
      "Total Parameters: 66,559,744\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 344.58 MB\n",
      "Model Memory Footprint: 266.85 MB\n",
      "\n",
      "Tokenization Time: 1.74 ms\n",
      "Forward Pass Time (BF16): 29.15 ms\n",
      "Total Pipeline Time: 30.90 ms\n",
      "Peak GPU Memory Used During Run: 446.48 MB\n",
      "\n",
      "MODEL: ClipMiniLM\n",
      "Total Parameters: 22,811,776\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 169.64 MB\n",
      "Model Memory Footprint: 91.91 MB\n",
      "\n",
      "Tokenization Time: 0.90 ms\n",
      "Forward Pass Time (BF16): 11.93 ms\n",
      "Total Pipeline Time: 12.84 ms\n",
      "Peak GPU Memory Used During Run: 195.59 MB\n",
      "\n",
      "VISION MODEL: ClipVitTiny\n",
      "Total Parameters: 5,573,824\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 100.04 MB\n",
      "Model Memory Footprint: 22.32 MB\n",
      "\n",
      "Preprocessing Time: 0.00 ms\n",
      "Forward Pass Time (BF16): 15.06 ms\n",
      "Total Pipeline Time: 15.07 ms\n",
      "Peak GPU Memory Used During Run: 128.76 MB\n",
      "\n",
      "VISION MODEL: ClipVitSmall\n",
      "Total Parameters: 21,764,224\n",
      "GPU Memory Before Loading: 77.73 MB\n",
      "GPU Memory After Loading: 165.44 MB\n",
      "Model Memory Footprint: 87.71 MB\n",
      "\n",
      "Preprocessing Time: 0.00 ms\n",
      "Forward Pass Time (BF16): 36.51 ms\n",
      "Total Pipeline Time: 36.51 ms\n",
      "Peak GPU Memory Used During Run: 244.42 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_information()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd5fc8",
   "metadata": {},
   "source": [
    "### Text Encoder Comparison\n",
    "\n",
    "DistilBert vs MiniLM\n",
    "Parameters: 66M vs 22M\n",
    "Forward pass time: 29.15 vs 11.93 ms\n",
    "Memory Footprint: 266.85 vs 91.91 MB\n",
    "\n",
    "Due to the size of MiniLM, it may struggle to have a similar expressive capability as DistilBert.\n",
    "\n",
    "MiniLM is 2.4x faster and 2.9x lighter in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dacb5",
   "metadata": {},
   "source": [
    "### Vision Encoder Comparison\n",
    "\n",
    "VitTiny vs VitSmall\n",
    "Parameters: 5.5M vs 21.8M\n",
    "Forward Pass: 15.06 ms vs 36.51ms\n",
    "Memory Footprint: 22.32 MB vs 87.71 MB\n",
    "\n",
    "VitTiny is extremely small which is potentially concerning. Most likely, ViTSmall will end up being used or even potentially upscaling to the next size.\n",
    "\n",
    "ViTTiny is 2.4x faster and 3.9x lighter in memory. If ViTTiny works, it will be very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b04587",
   "metadata": {},
   "source": [
    "### Determining if a model works\n",
    "\n",
    "1. Compare the gradient norms. If one encoder receives much smaller gradients than the other, its capacity is too small and the other encoder is overcompensating.\n",
    "2. Can Freeze one encoder and see how the loss changes. If the loss barely decreases, the non-frozen model is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c041d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b75b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_hyperparams = {\n",
    "    \"projection_dim\": 256,\n",
    "    \"max_text_len\": 64,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.98,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"batch_size\": 512,\n",
    "    \"minibatches\": 2,\n",
    "    \"temperature_init\": 0.07,\n",
    "    \"warmup_steps\": 200,\n",
    "    \"total_steps\": 1000,\n",
    "    \"enable_sdpa\": True,\n",
    "    \"amp\": True,\n",
    "    \"device\": \"cuda\",\n",
    "    \"precision\": \"bfloat16\"\n",
    "}\n",
    "\n",
    "def InfoNCE_Loss(image_embeddings, text_embeddings, logit_scale):\n",
    "    # L2 Norm the embeddings to prevent explosions. (batch, embedding_dim) so reshape on embedding_dim.\n",
    "    image_embeddings = F.normalize(image_embeddings, dim=-1)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "    logits = logit_scale * (image_embeddings @ text_embeddings.t())\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "def train_one_model(vision_encoder, text_encoder, hyperparameters, training_text_encoder=True):\n",
    "    training_model = text_encoder if training_text_encoder else vision_encoder\n",
    "    frozen_model = text_encoder if not training_text_encoder else vision_encoder\n",
    "    logit_scale = nn.Parameter(torch.tensor([torch.log(torch.tensor(1/untuned_hyperparams[\"temperature_init\"]))]))\n",
    "    # Add logit scale to this somehow. Clip Class? Probably what is best\n",
    "    optimizer = AdamW(\n",
    "        training_model.parameters(),\n",
    "        lr=untuned_hyperparams[\"lr\"],\n",
    "        weight_decay=untuned_hyperparams[\"weight_decay\"],\n",
    "        betas=(untuned_hyperparams[\"beta1\"], untuned_hyperparams[\"beta2\"])\n",
    "    )\n",
    "    scaler = torch.amp.grad_scaler.GradScaler()\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=untuned_hyperparams[\"warmup_steps\"],\n",
    "        num_training_steps=untuned_hyperparams[\"total_steps\"]\n",
    "    )\n",
    "    for batch in untuned_hyperparams[\"total_steps\"]:\n",
    "        for minibatch in  untuned_hyperparams[\"mini_batches\"]:\n",
    "            # Get minibatch\n",
    "            batch = None\n",
    "            batch_images = torch.tensor([])\n",
    "            batch_text = torch.tensor({})\n",
    "            batch_text = text_encoder.tokenizer(\n",
    "                batch_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=untuned_hyperparams[\"max_text_len\"],\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                image_embeddings = vision_encoder(batch_images)\n",
    "                text_embeddings = text_encoder(batch_text)\n",
    "        loss = InfoNCE_Loss(image_embeddings, text_embeddings, logit_scale)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(training_model.parameters(), untuned_hyperparams[\"grad_clip\"])\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if batch % 20 == 0:\n",
    "            print(f\"step {batch} / {untuned_hyperparams['total_steps']}, loss={loss.item():.4f}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
